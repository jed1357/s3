{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import *\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "                .appName(\"Stack Overflow Data Wrangling\")\n",
    "                .config(\"spark.jars\", \"postgresql-42.2.8.jar\") \n",
    "                .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = spark.read.csv(\n",
    "    \"questions.csv\",\n",
    "    header=True, inferSchema=True\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+--------------------+------------------+-----+----------+-------------+--------------------+\n",
      "|        questions_id|question_user_id|               title|                body|accepted_answer_id|score|view_count|comment_count|questions_created_at|\n",
      "+--------------------+----------------+--------------------+--------------------+------------------+-----+----------+-------------+--------------------+\n",
      "|            54233315|         1118630|XPath parent node...|<p>I'm trying to ...|              null| null|      null|         null|                null|\n",
      "|<p>So in Python I...|            null|                null|                null|              null| null|      null|         null|                null|\n",
      "|<pre><code>for ph...|            null|                null|                null|              null| null|      null|         null|                null|\n",
      "|       </code></pre>|            null|                null|                null|              null| null|      null|         null|                null|\n",
      "|  <p>to do that.</p>|            null|                null|                null|              null| null|      null|         null|                null|\n",
      "+--------------------+----------------+--------------------+--------------------+------------------+-----+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = questions.withColumnRenamed('id', 'questions_id') \n",
    "questions = questions.withColumnRenamed('user_id', 'question_user_id')\n",
    "questions = questions.withColumnRenamed('created_at', 'questions_created_at')\n",
    "questions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = spark.read.csv(\n",
    "      \"answers.csv\",\n",
    "    header=True, inferSchema=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-----------+--------------------+-------------+---------------------+------------------+\n",
      "|           answer_id|answer_user_id|question_id|        answers_body|answers_score|answers_comment_count|answers_created_at|\n",
      "+--------------------+--------------+-----------+--------------------+-------------+---------------------+------------------+\n",
      "|            53999517|       1771994|   53999275|<p>The <code>for....|         null|                 null|              null|\n",
      "|<p><div class=\"\"s...|          null|       null|                null|         null|                 null|              null|\n",
      "|<div class=\"\"snip...|          null|       null|                null|         null|                 null|              null|\n",
      "|<pre class=\"\"snip...|          null|       null|                null|         null|                 null|              null|\n",
      "|  const quotes = ...|          null|       null|                null|         null|                 null|              null|\n",
      "+--------------------+--------------+-----------+--------------------+-------------+---------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = answers.withColumnRenamed('id', 'answer_id') \n",
    "answers = answers.withColumnRenamed('user_id', 'answer_user_id')\n",
    "answers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.read.csv(\n",
    "    \"users.csv\",\n",
    "    header=True, inferSchema=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.withColumnRenamed('id', 'user_id')\n",
    "# users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select users from only one country of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------+--------------------+--------+--------------------+-----+--------+----------+--------------------+-------------------+-------------------+----+----+\n",
      "| user_id|    display_name|reputation|         website_url|location|            about_me|views|up_votes|down_votes|           image_url|         created_at|         updated_at|_c12|_c13|\n",
      "+--------+----------------+----------+--------------------+--------+--------------------+-----+--------+----------+--------------------+-------------------+-------------------+----+----+\n",
      "|10260337|Pratik K. Tiwari|         1|http://pratiktiwa...|   India|<p>Hi, I am first...| null|    null|      null|                null|               null|               null|null|null|\n",
      "| 8387608|           Abhay|         1|                null|   India|                null|    0|       0|         0|https://www.grava...|2017-07-29 22:20:14|2019-09-06 16:03:34|null|null|\n",
      "| 4712224|    DevallaVamsi|         1|                null|   India|                null|    0|       0|         0|https://www.grava...|2015-03-25 13:20:41|2019-06-26 15:22:39|null|null|\n",
      "|10268565|           sagar|         1|                null|   India|                null|    5|       0|         0|https://www.grava...|2018-08-24 08:15:37|2019-05-23 16:30:43|null|null|\n",
      "| 8403502|          iamnav|         1|                null|   India|                null|    1|       0|         0|https://www.grava...|2017-08-02 05:56:19|2019-10-12 20:17:03|null|null|\n",
      "+--------+----------------+----------+--------------------+--------+--------------------+-----+--------+----------+--------------------+-------------------+-------------------+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "users.registerTempTable('users')\n",
    "userCountry = spark.sql(\"SELECT * FROM users WHERE location = 'India'\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users = users.withColumn('location', regexp_replace('location', 'USA', 'United States'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiaTb = users.filter(users.location.contains('India'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiaTb = indiaTb.withColumn('location', lower(col('location')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+----------+--------------------+--------------------+--------------------+-----+--------+----------+--------------------+-------------------+-------------------+----+----+\n",
      "| user_id|      display_name|reputation|         website_url|            location|            about_me|views|up_votes|down_votes|           image_url|         created_at|         updated_at|_c12|_c13|\n",
      "+--------+------------------+----------+--------------------+--------------------+--------------------+-----+--------+----------+--------------------+-------------------+-------------------+----+----+\n",
      "| 8357266|            suryan|         7|https://twitter.c...|    bangalore, india|                null|    8|       0|         0|https://www.grava...|2017-07-24 10:55:23|2019-06-19 05:00:16|null|null|\n",
      "| 6504306|             A.Raw|         4|                null|    new delhi, india|                null|   10|       0|         0|https://i.stack.i...|2016-06-23 12:58:03|2019-10-12 06:59:32|null|null|\n",
      "|10260743|     Kartik Juneja|         3|                null|    gharaunda, india|<p>4th year Btech...| null|    null|      null|                null|               null|               null|null|null|\n",
      "| 4689205|            sd5869|         1|    http://sd5869.me|    new delhi, india|                null|    5|       0|         0|                null|2015-03-19 10:20:21|2019-09-18 14:36:03|null|null|\n",
      "|10262756|           Ali Mir|         5|    http://nowebsite|    jalandhar, india|<p>I am a profess...| null|    null|      null|                null|               null|               null|null|null|\n",
      "|10262983|     Nikunj Hapani|         1|                null|        surat, india|<p>Hello, I am PH...| null|    null|      null|                null|               null|               null|null|null|\n",
      "|10249338|Prashant Kalikotay|         3|                null|darjeeling, west ...|<p>Currently: Pur...| null|    null|      null|                null|               null|               null|null|null|\n",
      "|10254503|      Akshay Nikam|         6|                null|         pune, india|                null|    6|       0|         0|https://lh4.googl...|2018-08-21 11:23:31|2019-08-22 06:36:53|null|null|\n",
      "|10254796|            Harsh |         6|                null|       mumbai, india|                null|    4|       0|         0|https://lh4.googl...|2018-08-21 12:33:09|2019-08-21 10:48:38|null|null|\n",
      "| 8363933|    Jitendra Kumar|         1|                null|    bangalore, india|                null|    1|       0|         0|https://i.stack.i...|2017-07-25 13:15:23|2019-10-09 07:36:13|null|null|\n",
      "+--------+------------------+----------+--------------------+--------------------+--------------------+-----+--------+----------+--------------------+-------------------+-------------------+----+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indiaTb = indiaTb.withColumn('location', regexp_replace('location', r\"[,]\\s*\\w*\\s*[,]\", ','))\n",
    "indiaTb.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the country and city into new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----+----------+-------------------+-------------------+-------------------+---------+-------+\n",
      "| user_id| display_name|views|reputation|         updated_at|           location|         created_at|     city|country|\n",
      "+--------+-------------+-----+----------+-------------------+-------------------+-------------------+---------+-------+\n",
      "| 8357266|       suryan|    8|         7|2019-06-19 05:00:16|[bangalore,  india]|2017-07-24 10:55:23|bangalore|  india|\n",
      "| 6504306|        A.Raw|   10|         4|2019-10-12 06:59:32|[new delhi,  india]|2016-06-23 12:58:03|new delhi|  india|\n",
      "|10260743|Kartik Juneja| null|         3|               null|[gharaunda,  india]|               null|gharaunda|  india|\n",
      "| 4689205|       sd5869|    5|         1|2019-09-18 14:36:03|[new delhi,  india]|2015-03-19 10:20:21|new delhi|  india|\n",
      "|10262756|      Ali Mir| null|         5|               null|[jalandhar,  india]|               null|jalandhar|  india|\n",
      "+--------+-------------+-----+----------+-------------------+-------------------+-------------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indialoc = indiaTb.withColumn('location', split(indiaTb.location, ',')) \\\n",
    "  .select('user_id', 'display_name', 'views', 'reputation', 'updated_at', 'location', 'created_at', element_at(col('location'),-2).alias('city'), element_at(col('location'), -1).alias('country'))\n",
    "\n",
    "indialoc.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join this with the questions and only pick questions with at least 20 view_counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join this with the questions and only pick questions with at least 20 view_counts.\n",
    "indialoc.registerTempTable('indialoc')\n",
    "questions.registerTempTable('questions')\n",
    "\n",
    "quesIndia = spark.sql(\"SELECT * FROM indialoc LEFT JOIN questions ON (indialoc.user_id = questions.question_user_id) WHERE questions.view_count >= 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+----------+-------------------+-------------------+-------------------+---------+-------+------------+----------------+-----+----+------------------+-------+----------+-------------+--------------------+\n",
      "|user_id|display_name|views|reputation|         updated_at|           location|         created_at|     city|country|questions_id|question_user_id|title|body|accepted_answer_id|  score|view_count|comment_count|questions_created_at|\n",
      "+-------+------------+-----+----------+-------------------+-------------------+-------------------+---------+-------+------------+----------------+-----+----+------------------+-------+----------+-------------+--------------------+\n",
      "|3273751| user3273751|    5|        16|2019-07-09 07:35:59|[bangalore,  india]|2014-02-05 05:16:17|bangalore|  india|           R|         3273751|46609|6452|              3447|1810631|     45933|         6382|                3447|\n",
      "+-------+------------+-----+----------+-------------------+-------------------+-------------------+---------+-------+------------+----------------+-----+----+------------------+-------+----------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quesIndia.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quesIndia.filter(quesIndia.display_name.isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = answers.withColumnRenamed('body', 'answers_body') \n",
    "answers = answers.withColumnRenamed('score', 'answers_score') \n",
    "answers = answers.withColumnRenamed('comment_count', 'answers_comment_count') \n",
    "answers = answers.withColumnRenamed('created_at', 'answers_created_at') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the answers to the results of (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "quesIndia.registerTempTable('quesIndia')\n",
    "answers.registerTempTable('answers')\n",
    "\n",
    "ansRes = spark.sql(\"SELECT * FROM quesIndia LEFT JOIN answers ON (answers.question_id = quesIndia.questions_id)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ansRes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this to return the minimum updated_at time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    min(updated_at)|\n",
      "+-------------------+\n",
      "|2019-07-09 07:35:59|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_ansRes = ansRes.select([min('updated_at')])\n",
    "min_ansRes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use spark to write the results into this table with the snippet below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o299.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 129 in stage 66.0 failed 1 times, most recent failure: Lost task 129.0 in stage 66.0 (TID 1568, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"user_id\",\"display_name\",\"views\",\"reputation\",\"updated_at\",\"location\",\"created_at\",\"city\",\"country\",\"questions_id\",\"question_user_id\",\"title\",\"body\",\"accepted_answer_id\",\"score\",\"view_count\",\"comment_count\",\"questions_created_at\",\"answer_id\",\"answer_user_id\",\"question_id\",\"answers_body\",\"answers_score\",\"answers_comment_count\",\"answers_created_at\") VALUES ('3273751','user3273751','5','16','2019-07-09 07:35:59','{\"bangalore\",\" india\"}','2014-02-05 05:16:17','bangalore',' india','R','3273751','46609','6452','3447','1810631','45933','6382','3447','<p>Just for clarification; <code>P','p','R','r','S','s</code> are called identifiers',' and you need <code>float</code> value to assign them with(since they are of type<code>float</code>).</p>') was aborted: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:837)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:834)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"user_id\",\"display_name\",\"views\",\"reputation\",\"updated_at\",\"location\",\"created_at\",\"city\",\"country\",\"questions_id\",\"question_user_id\",\"title\",\"body\",\"accepted_answer_id\",\"score\",\"view_count\",\"comment_count\",\"questions_created_at\",\"answer_id\",\"answer_user_id\",\"question_id\",\"answers_body\",\"answers_score\",\"answers_comment_count\",\"answers_created_at\") VALUES ('3273751','user3273751','5','16','2019-07-09 07:35:59','{\"bangalore\",\" india\"}','2014-02-05 05:16:17','bangalore',' india','R','3273751','46609','6452','3447','1810631','45933','6382','3447','<p>Just for clarification; <code>P','p','R','r','S','s</code> are called identifiers',' and you need <code>float</code> value to assign them with(since they are of type<code>float</code>).</p>') was aborted: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:837)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:834)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-bca26ea838cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'postgres1234'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdbtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stackoverflow_filtered.results'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m ).save(mode='append')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o299.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 129 in stage 66.0 failed 1 times, most recent failure: Lost task 129.0 in stage 66.0 (TID 1568, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"user_id\",\"display_name\",\"views\",\"reputation\",\"updated_at\",\"location\",\"created_at\",\"city\",\"country\",\"questions_id\",\"question_user_id\",\"title\",\"body\",\"accepted_answer_id\",\"score\",\"view_count\",\"comment_count\",\"questions_created_at\",\"answer_id\",\"answer_user_id\",\"question_id\",\"answers_body\",\"answers_score\",\"answers_comment_count\",\"answers_created_at\") VALUES ('3273751','user3273751','5','16','2019-07-09 07:35:59','{\"bangalore\",\" india\"}','2014-02-05 05:16:17','bangalore',' india','R','3273751','46609','6452','3447','1810631','45933','6382','3447','<p>Just for clarification; <code>P','p','R','r','S','s</code> are called identifiers',' and you need <code>float</code> value to assign them with(since they are of type<code>float</code>).</p>') was aborted: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:837)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:834)\n\t... 16 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow_filtered.results (\"user_id\",\"display_name\",\"views\",\"reputation\",\"updated_at\",\"location\",\"created_at\",\"city\",\"country\",\"questions_id\",\"question_user_id\",\"title\",\"body\",\"accepted_answer_id\",\"score\",\"view_count\",\"comment_count\",\"questions_created_at\",\"answer_id\",\"answer_user_id\",\"question_id\",\"answers_body\",\"answers_score\",\"answers_comment_count\",\"answers_created_at\") VALUES ('3273751','user3273751','5','16','2019-07-09 07:35:59','{\"bangalore\",\" india\"}','2014-02-05 05:16:17','bangalore',' india','R','3273751','46609','6452','3447','1810631','45933','6382','3447','<p>Just for clarification; <code>P','p','R','r','S','s</code> are called identifiers',' and you need <code>float</code> value to assign them with(since they are of type<code>float</code>).</p>') was aborted: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:837)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1546)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:834)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: column \"user_id\" is of type integer but expression is of type character varying\n  Hint: You will need to rewrite or cast the expression.\n  Position: 403\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2497)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2233)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:310)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:834)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ansRes.write.format(\"jdbc\").options(\n",
    "    url='jdbc:postgresql://localhost/postgres',\n",
    "    driver='org.postgresql.Driver',\n",
    "    user='postgres',\n",
    "    password='postgres1234',\n",
    "    dbtable='stackoverflow_filtered.results'\n",
    ").save(mode='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansRes.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In your Jupyter notebook, state the difference between views and materialized views\n",
    "\n",
    "The difference between a view and a materialised view is that a view serves as a virtual table with the query passed where as the materialised view serves as a physical store table for the query passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you were to give an award to one user, who will it be? And why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would give it to user because he is the only user in my table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
